{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WEEK5 NAIVE BAYES@GOD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3NB8ilK0iYK"
      },
      "source": [
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import gc\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmFUhtWrQtZ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "% cd /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1mXTAMF2rZ4",
        "outputId": "76185df0-04e1-4fd3-adad-f7e515ce33a5"
      },
      "source": [
        "# del doc_word_mapper\n",
        "gc.collect() # in case if ram keeps increasing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3wSAnvF3C44"
      },
      "source": [
        "label_count_mapper = np.zeros((20, 20)) # since i know total number of docs I'm making this array directly\n",
        "class_docs = {} # how much docs each class has\n",
        "train_y = [] # training labels (1-20)\n",
        "conf_matrix = np.zeros((20, 20))\n",
        "\n",
        "with open('./news_data/train.label', 'r') as fp:\n",
        "  for line in fp:\n",
        "    label = int(line.strip())\n",
        "    # its good to point out that doc_id 'n' will have label at 'n-1'th place\n",
        "    # in simple words doc0 will have label at train_y[0] which will be between 1-20\n",
        "    train_y.append(label)\n",
        "    class_docs[label] = class_docs.get(label, 0) + 1\n",
        "    \n",
        "previous_doc = 1\n",
        "freq = {} # (word, class): count\n",
        "with open('./news_data/train.data', 'r') as fp:\n",
        "  word_ids = [] # this will basically contain the words and their counts\n",
        "  for line in fp:\n",
        "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
        "    # this means we have got all the words of our doc now we can take care of frequencies\n",
        "    # for doc1\n",
        "    # (1,5), (2,10), ...\n",
        "    # doc1 => class1\n",
        "    if previous_doc != doc_id:\n",
        "      # doc N will have label at (N-1)th place\n",
        "      class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
        "      for word, word_count in word_ids:\n",
        "        # we are gonna have count of each word in each class in this dictionary\n",
        "        # example. hello, class1 => 20 times\n",
        "        freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
        "      previous_doc = doc_id\n",
        "      word_ids = [(word_id, count)]\n",
        "    else:\n",
        "      word_ids.append((word_id, count)) # appending word_id\n",
        "# for the last doc\n",
        "class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
        "for word, count in word_ids:\n",
        "  freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
        "del word_ids\n",
        "\n",
        "vocab = set([pair[0] for pair in freq.keys()])\n",
        "v_len = len(vocab)\n",
        "\n",
        "# finding how much words each class has\n",
        "class_words = {}\n",
        "for pair, word_count in freq.items():\n",
        "  class_ = pair[1]\n",
        "  class_words[class_] = class_words.get(class_, 0) + word_count\n",
        "\n",
        "total_docs = len(train_y) # this should be 11269 if using their indexing (see train.data)\n",
        "# sum(class_docs.values()) # again, this should be same as total_docs\n",
        "\n",
        "prob_class = {}\n",
        "prob_word_class = {}\n",
        "\n",
        "# finding probability of each class\n",
        "for i in class_docs:\n",
        "  prob_class[i] = class_docs[i]/total_docs\n",
        "\n",
        "# findinf probability of each word in each class\n",
        "# we are doing this with smoothing too, for better results\n",
        "for word in vocab:\n",
        "  for class_ in class_words:\n",
        "    freq_class = freq.get((word, class_), 0)\n",
        "    # word/class\n",
        "    prob_word_class[(word, class_)] = (freq_class + 1)/(class_words[class_] + v_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eyuRkGr7Unc"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "so now \n",
        "we have prior probability of each class => prob_class\n",
        "we have total count of each class => label_count_mapper\n",
        "\n",
        "doc: word1 word2 ...\n",
        "prob[class/givenwords] = prob[class] * prob[word1/class] * prob[word2/class]....\n",
        "\n",
        "and find max probability to assign the class\n",
        "\n",
        "Problem: because docs are big we have hell of words\n",
        "so our probabilities became very very very small(which computer considers as 0)\n",
        "so we will consider taking log on both sides\n",
        "\n",
        "log(prob[class]) + log(prob[word1/class]) + log(prob[word2/class])....\n",
        "\n",
        "moreover we also have occurencies of each word.\n",
        "so\n",
        "prob = prob[class] * (prob[word1/class] * prob[word1/class]... )N times....\n",
        "so\n",
        "prob = prob[class] * prob[word1/class]^occurence_word1 * prob[word2/class]^occurence_word2\n",
        "\n",
        "hence\n",
        "log(prob) = log(prob[class]) + (occurence_word1 * log(word1/class)) + ...\n",
        "we do this for all 20 classes\n",
        "and see which has biggest value\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJbPa51ZOZBU",
        "outputId": "fd2ff1ed-62ac-421e-9b6b-4a09d5417454"
      },
      "source": [
        "conf_matrix = np.zeros((20,20))\n",
        "previous_doc = 1\n",
        "y_expected = []\n",
        "y_actual = []\n",
        "with open('./news_data/test.label', 'r') as fp:\n",
        "  for line in fp:\n",
        "    y_expected.append(int(line.strip()))\n",
        "total_test_docs = len(y_expected)\n",
        "correct_classified = 0\n",
        "with open('./news_data/test.data', 'r') as fp:\n",
        "  word_ids = [] # this will basically contain the words and their counts\n",
        "  j = 0\n",
        "  for line in fp:\n",
        "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
        "    if previous_doc != doc_id:\n",
        "      probs = deepcopy(prob_class)\n",
        "      for i in probs:\n",
        "        probs[i] = np.log(probs[i])\n",
        "      for word, word_count in word_ids:\n",
        "        for class_ in range(1,21):\n",
        "          # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
        "          probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
        "          # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
        "      _max_class = 1\n",
        "      _max_val = - np.inf\n",
        "      for i in probs:\n",
        "        if probs[i] > _max_val:\n",
        "          _max_val = probs[i]\n",
        "          _max_class = i\n",
        "      y_actual.append(_max_class)\n",
        "      # print(_max_class, end = ' ')\n",
        "      if y_expected[j] == _max_class:\n",
        "        correct_classified+=1\n",
        "      conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
        "      j += 1\n",
        "      previous_doc = doc_id\n",
        "      word_ids = [(word_id, count)]\n",
        "    else:\n",
        "      word_ids.append((word_id, count)) # appending word_id\n",
        "\n",
        "# this is for the last word_ids (code can be taken into function to remove redunduncy)\n",
        "for word, word_count in word_ids:\n",
        "  for class_ in range(1,21):\n",
        "    # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
        "    probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
        "    # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
        "_max_class = 1\n",
        "_max_val = - np.inf\n",
        "for i in probs:\n",
        "  if probs[i] > _max_val:\n",
        "    _max_val = probs[i]\n",
        "    _max_class = i\n",
        "# print(_max_class, end = ' ')\n",
        "y_actual.append(_max_class)\n",
        "if y_expected[j] == _max_class:\n",
        "  correct_classified+=1\n",
        "conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
        "\n",
        "incorrect_classified = total_test_docs - correct_classified\n",
        "print(correct_classified/total_test_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7846768820786143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4w3oTLyWjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d30608f-03c9-474d-beae-3442ef5bdc34"
      },
      "source": [
        "# # FOR F1 score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_expected, y_actual, average=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72617247, 0.71411339, 0.64806202, 0.67709497, 0.7537415 ,\n",
              "       0.79947575, 0.734375  , 0.8436019 , 0.9121447 , 0.91578947,\n",
              "       0.95037221, 0.82045455, 0.71625344, 0.85375494, 0.86709677,\n",
              "       0.79704017, 0.77234803, 0.87601078, 0.58730159, 0.50969529])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ1GTLKXB1Mh",
        "outputId": "57fc9666-29f6-41a2-b9c1-2b67f0cc2e77"
      },
      "source": [
        "conf_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[240.,   3.,   3.,   0.,   1.,   0.,   0.,   0.,   0.,   4.,   2.,\n",
              "          0.,   2.,  10.,   4.,   7.,   1.,  12.,   7.,  47.],\n",
              "       [  0., 296.,  33.,   8.,   8.,  42.,   9.,   1.,   1.,   0.,   0.,\n",
              "          4.,  17.,   7.,   8.,   2.,   0.,   1.,   1.,   2.],\n",
              "       [  0.,   6., 209.,  15.,   9.,   8.,   4.,   0.,   0.,   0.,   0.,\n",
              "          1.,   0.,   1.,   0.,   1.,   0.,   0.,   0.,   0.],\n",
              "       [  0.,  12.,  60., 303.,  36.,  11.,  46.,   2.,   0.,   1.,   0.,\n",
              "          1.,  28.,   3.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
              "       [  0.,   8.,  10.,  22., 277.,   2.,  21.,   0.,   0.,   1.,   0.,\n",
              "          2.,   7.,   0.,   0.,   1.,   1.,   0.,   0.,   0.],\n",
              "       [  1.,  21.,  30.,   2.,   2., 305.,   0.,   1.,   0.,   3.,   0.,\n",
              "          1.,   3.,   0.,   1.,   2.,   0.,   0.,   1.,   0.],\n",
              "       [  0.,   1.,   0.,   5.,   5.,   1., 235.,   5.,   1.,   2.,   0.,\n",
              "          1.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,   0.],\n",
              "       [  0.,   3.,   1.,   6.,   4.,   0.,  31., 356.,  25.,   3.,   1.,\n",
              "          0.,   9.,   4.,   0.,   0.,   3.,   2.,   1.,   0.],\n",
              "       [  0.,   2.,   2.,   0.,   1.,   2.,   5.,   5., 353.,   1.,   0.,\n",
              "          0.,   2.,   0.,   1.,   0.,   1.,   1.,   0.,   1.],\n",
              "       [  0.,   0.,   2.,   0.,   1.,   1.,   0.,   2.,   2., 348.,   4.,\n",
              "          0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.],\n",
              "       [  1.,   0.,   1.,   1.,   0.,   0.,   1.,   0.,   0.,  16., 383.,\n",
              "          0.,   1.,   0.,   1.,   0.,   1.,   1.,   0.,   0.],\n",
              "       [  1.,  17.,  16.,   5.,   4.,  10.,   3.,   1.,   1.,   2.,   0.,\n",
              "        361.,  46.,   1.,   4.,   1.,   3.,   4.,   3.,   2.],\n",
              "       [  1.,   4.,   1.,  24.,  16.,   0.,   9.,   5.,   1.,   2.,   0.,\n",
              "          3., 260.,   3.,   4.,   0.,   0.,   0.,   0.,   0.],\n",
              "       [  2.,   3.,   4.,   0.,   8.,   0.,   2.,   0.,   1.,   0.,   2.,\n",
              "          2.,   6., 324.,   4.,   1.,   1.,   0.,   3.,   3.],\n",
              "       [  3.,   7.,   4.,   1.,   2.,   3.,   3.,   2.,   0.,   0.,   1.,\n",
              "          0.,   4.,   3., 336.,   0.,   2.,   0.,   7.,   5.],\n",
              "       [ 42.,   4.,   5.,   0.,   0.,   1.,   4.,   1.,   1.,   3.,   2.,\n",
              "          2.,   5.,  17.,   4., 377.,   3.,   7.,   2.,  68.],\n",
              "       [  4.,   0.,   0.,   0.,   3.,   1.,   1.,   5.,   4.,   1.,   0.,\n",
              "          8.,   0.,   3.,   1.,   3., 324.,   3.,  95.,  19.],\n",
              "       [  7.,   1.,   0.,   0.,   0.,   1.,   3.,   1.,   2.,   2.,   1.,\n",
              "          0.,   2.,   7.,   2.,   1.,   2., 325.,   4.,   5.],\n",
              "       [  7.,   1.,   9.,   0.,   6.,   2.,   5.,   8.,   5.,   8.,   3.,\n",
              "          8.,   0.,   9.,  21.,   1.,  16.,  19., 185.,   7.],\n",
              "       [  9.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "          1.,   0.,   0.,   1.,   1.,   4.,   0.,   1.,  92.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK3-apqk1Pgr"
      },
      "source": [
        "# ROCHIO Algos\n",
        "# each doc will be of N space where N is vocab length(set of unique words allover)\n",
        "\n",
        "# first we find centroid of each class\n",
        "# so say to find centroid of class 1, we find average N dimensional vector of documents of class 1 in training\n",
        "# we can say this is end of training phase\n",
        "\n",
        "# now take a doc from test phase and find its cosin similarity with each class centroid\n",
        "# and we give it label of one which is of closest angle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCH7HUHEUhtq",
        "outputId": "a244644b-a483-4213-80e9-8180509c3189"
      },
      "source": [
        "text = \"\"\"Let me take you down\n",
        "'Cause I'm going to Strawberry Fields\n",
        "Nothing is real\n",
        "And nothing to get hung about\n",
        "Strawberry Fields forever\n",
        "Living is easy with eyes closed\n",
        "Misunderstanding all you see\n",
        "It's getting hard to be someone\n",
        "But it all works out\n",
        "It doesn't matter much to me\n",
        "Let me take you down\n",
        "'Cause I'm going to Strawberry Fields\n",
        "Nothing is real\n",
        "And nothing to get hung about\n",
        "Strawberry Fields forever\n",
        "No one I think is in my tree\n",
        "I mean it must be high or low\n",
        "That is you can't, you know, tune in\n",
        "But it's all right\n",
        "That is I think it's not too bad\n",
        "Let me take you down\n",
        "'Cause I'm going to Strawberry Fields\n",
        "Nothing is real\n",
        "And nothing to get hung about\n",
        "Strawberry Fields forever\n",
        "Always, no sometimes, think it's me\n",
        "But you know I know when it's a dream\n",
        "I think I know I mean a yes\n",
        "But it's all wrong\n",
        "That is I think I disagree\n",
        "Let me take you down\n",
        "'Cause I'm going to Strawberry Fields\n",
        "Nothing is real\n",
        "And nothing to get hung about\n",
        "Strawberry Fields forever\n",
        "Strawberry Fields forever\n",
        "Strawberry Fields forever\"\"\"\n",
        "\n",
        "def get_index(char: str) -> int:\n",
        "    if char.isalpha():\n",
        "      return ord(char.lower()) - 97 # so a comes on 0, b-1, ...z-25\n",
        "    else:\n",
        "      return 26\n",
        "\n",
        "def get_occurence_matrix(text):\n",
        "  occurence_matrix = np.zeros((27,27))\n",
        "  l = len(text)\n",
        "  # a => 97, z => 122\n",
        "  for i in range(1, l):\n",
        "    current = text[i-1]\n",
        "    next = text[i]\n",
        "    current_index = get_index(current)\n",
        "    next_index = get_index(next)\n",
        "    occurence_matrix[current_index, next_index] += 1\n",
        "  return occurence_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybtc09xZVi6m",
        "outputId": "f33888ea-efb4-4e15-9459-b76867737054"
      },
      "source": [
        "occurence_matrix = get_occurence_matrix(text)\n",
        "def get_probability_matrix(occurence_matrix):\n",
        "  prob_matrix =  occurence_matrix/occurence_matrix.sum(axis=1, keepdims=True)\n",
        "  prob_matrix[np.isnan(prob_matrix)] = 0 # removing NaNs\n",
        "  return prob_matrix\n",
        "get_probability_matrix(occurence_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.07843137, 0.        , 0.01960784, 0.        ,\n",
              "        0.        , 0.01960784, 0.        , 0.        , 0.        ,\n",
              "        0.07843137, 0.17647059, 0.01960784, 0.15686275, 0.        ,\n",
              "        0.        , 0.        , 0.01960784, 0.01960784, 0.07843137,\n",
              "        0.07843137, 0.        , 0.19607843, 0.        , 0.01960784,\n",
              "        0.        , 0.03921569],\n",
              "       [0.04761905, 0.        , 0.        , 0.        , 0.57142857,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.19047619,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.19047619, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.71428571, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.14285714, 0.        , 0.        ,\n",
              "        0.        , 0.14285714, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.03846154,\n",
              "        0.        , 0.        , 0.        , 0.07692308, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.19230769,\n",
              "        0.        , 0.        , 0.03846154, 0.38461538, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.26923077],\n",
              "       [0.09411765, 0.        , 0.        , 0.01176471, 0.03529412,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.11764706, 0.        , 0.01176471, 0.01176471,\n",
              "        0.        , 0.        , 0.21176471, 0.04705882, 0.11764706,\n",
              "        0.        , 0.07058824, 0.        , 0.        , 0.01176471,\n",
              "        0.        , 0.25882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.625     , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.375     ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.15625   ,\n",
              "        0.        , 0.        , 0.0625    , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.125     ,\n",
              "        0.        , 0.        , 0.03125   , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.625     ],\n",
              "       [0.14814815, 0.        , 0.        , 0.        , 0.03703704,\n",
              "        0.        , 0.        , 0.        , 0.51851852, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.03703704,\n",
              "        0.14814815, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.11111111],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.14285714,\n",
              "        0.        , 0.02857143, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01428571, 0.31428571, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.15714286, 0.14285714,\n",
              "        0.        , 0.01428571, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.18571429],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.28571429,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.28571429, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.07142857, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.35714286],\n",
              "       [0.        , 0.        , 0.        , 0.33333333, 0.13333333,\n",
              "        0.        , 0.        , 0.        , 0.03333333, 0.        ,\n",
              "        0.        , 0.13333333, 0.        , 0.        , 0.06666667,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.03333333, 0.        , 0.        ,\n",
              "        0.        , 0.26666667],\n",
              "       [0.04761905, 0.        , 0.        , 0.        , 0.52380952,\n",
              "        0.        , 0.        , 0.        , 0.04761905, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.0952381 , 0.        , 0.        , 0.        , 0.04761905,\n",
              "        0.        , 0.23809524],\n",
              "       [0.        , 0.        , 0.        , 0.1       , 0.05      ,\n",
              "        0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
              "        0.08333333, 0.        , 0.        , 0.        , 0.25      ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.18333333],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.015625  ,\n",
              "        0.        , 0.        , 0.        , 0.0625    , 0.        ,\n",
              "        0.        , 0.        , 0.03125   , 0.046875  , 0.015625  ,\n",
              "        0.        , 0.        , 0.125     , 0.015625  , 0.140625  ,\n",
              "        0.203125  , 0.        , 0.140625  , 0.        , 0.        ,\n",
              "        0.        , 0.203125  ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.18181818, 0.        , 0.        , 0.01818182, 0.23636364,\n",
              "        0.        , 0.        , 0.        , 0.01818182, 0.        ,\n",
              "        0.01818182, 0.        , 0.        , 0.        , 0.01818182,\n",
              "        0.        , 0.        , 0.18181818, 0.01818182, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.18181818,\n",
              "        0.        , 0.12727273],\n",
              "       [0.01851852, 0.        , 0.        , 0.        , 0.11111111,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.01851852, 0.03703704,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.22222222,\n",
              "        0.01851852, 0.        , 0.        , 0.        , 0.01851852,\n",
              "        0.        , 0.55555556],\n",
              "       [0.05952381, 0.        , 0.        , 0.        , 0.01190476,\n",
              "        0.        , 0.        , 0.20238095, 0.02380952, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.13095238,\n",
              "        0.        , 0.        , 0.13095238, 0.        , 0.02380952,\n",
              "        0.01190476, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.4047619 ],\n",
              "       [0.        , 0.        , 0.03448276, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.20689655, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.17241379, 0.31034483,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.27586207],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.85714286,\n",
              "        0.        , 0.        , 0.        , 0.14285714, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.04166667, 0.41666667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.04166667, 0.04166667, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.16666667, 0.04166667,\n",
              "        0.        , 0.        , 0.04166667, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.20833333],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.08695652,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.34782609,\n",
              "        0.        , 0.        , 0.        , 0.04347826, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.52173913],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.07042254, 0.03286385, 0.02816901, 0.03286385, 0.00938967,\n",
              "        0.07511737, 0.04225352, 0.02816901, 0.15492958, 0.        ,\n",
              "        0.01877934, 0.02347418, 0.07981221, 0.05164319, 0.01408451,\n",
              "        0.        , 0.        , 0.02347418, 0.08920188, 0.12676056,\n",
              "        0.        , 0.        , 0.01877934, 0.        , 0.04225352,\n",
              "        0.        , 0.03755869]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}